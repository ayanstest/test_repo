{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building predictive models with sklearn pipelines\n",
    "\n",
    "**Objective of the competition**:\n",
    "\n",
    "The competition dataset contains text from works of fiction written by spooky authors of the public domain:\n",
    "- Edgar Allan Poe (EAP)\n",
    "- HP Lovecraft (HPL)\n",
    "- Mary Wollstonecraft Shelley (MWS)\n",
    "\n",
    "The objective is to accurately identify the author of the sentences in the test set.\n",
    "\n",
    "**Objective of the notebook:**\n",
    "\n",
    "The goal of this notebook is to break down the pipeline process to make it easier to see how they all fit together.\n",
    "\n",
    "General description and data are available on [Kaggle](https://www.kaggle.com/c/spooky-author-identification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Load dataset](#1)\n",
    "\n",
    "2. [Preprocessing and Feature Engineering](#2)\n",
    "\n",
    "3. [Creating a Pipeline](#3)\n",
    "\n",
    "4. [Cross Validation To Find The Best Pipeline](#4)\n",
    "\n",
    "5. [Final Predictions](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">Load dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:51:23.912775Z",
     "start_time": "2020-04-29T07:51:22.454094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "df.dropna(axis=0)\n",
    "df.set_index(\"id\",inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <a name=\"2\">Preprocessing and Feature Engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's do some basic feature engineering. To make it easier to replicate on the submission data, we will encapsulate the logic into a function.\n",
    "\n",
    "Note, all of this preprocessing is standard stuff, and does not depend on the data it's processing on, so it's ok to do this now. Things like count vectorization and numeric scaling depend on the data it's run on, so that part must be done differently. We will get to that later.\n",
    "\n",
    "For now, we will count the number of words in each row, the number of characters, the number of non stop words, and the number of commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:51:44.421164Z",
     "start_time": "2020-04-29T07:51:42.457465Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>this process however afforded me no means of a...</td>\n",
       "      <td>224</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>6.380952</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>it never once occurred to me that the fumbling...</td>\n",
       "      <td>70</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>in his left hand was a gold snuff box from whi...</td>\n",
       "      <td>195</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>5.947368</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>how lovely is spring as we looked from windsor...</td>\n",
       "      <td>202</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>finding nothing else not even gold the superin...</td>\n",
       "      <td>170</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>7.187500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id26305  This process, however, afforded me no means of...    EAP   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                                 processed  length  words  \\\n",
       "id                                                                          \n",
       "id26305  this process however afforded me no means of a...     224     41   \n",
       "id17569  it never once occurred to me that the fumbling...      70     14   \n",
       "id11008  in his left hand was a gold snuff box from whi...     195     36   \n",
       "id27763  how lovely is spring as we looked from windsor...     202     34   \n",
       "id12958  finding nothing else not even gold the superin...     170     27   \n",
       "\n",
       "         words_not_stopword  avg_word_length  commas  \n",
       "id                                                    \n",
       "id26305                  21         6.380952       4  \n",
       "id17569                   6         6.166667       0  \n",
       "id11008                  19         5.947368       4  \n",
       "id27763                  21         6.476190       3  \n",
       "id12958                  16         7.187500       2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def processing(df):\n",
    "    # lowering and removing punctuation\n",
    "    df[\"processed\"]=df[\"text\"].apply(lambda x: re.sub(r\"[^\\w\\s]\",\"\",x.lower()))\n",
    "    \n",
    "    # numerical feature engineering\n",
    "    # total length of sentence\n",
    "    df[\"length\"]=df[\"processed\"].apply(lambda x: len(x))\n",
    "    # get number of words\n",
    "    df[\"words\"]=df[\"processed\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"words_not_stopword\"]=df[\"processed\"].apply(lambda x: len([t for t in x.split(\" \") if t not in stopwords]))\n",
    "    # get the average word length\n",
    "    df[\"avg_word_length\"]=df[\"processed\"].apply(lambda x: np.mean([len(t) for t in x.split(\" \") if t not in stopwords]) if len([len(t) for t in x.split(\" \") if t not in stopwords])>0 else 0)\n",
    "    df[\"commas\"]=df[\"text\"].apply(lambda x: x.count(\",\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df=processing(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <a name=\"3\">Creating a Pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sklearn pipeline functionality makes it easier to repeat commonly occurring steps in our modeling process. Similar to the processing function, it provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\n",
    "\n",
    "So let's build the pipelines up from the bottom. Since pipelines are made from pipelines, it's useful to see how they build on each other.\n",
    "\n",
    "First step, split data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T07:58:03.473338Z",
     "start_time": "2020-04-29T07:58:03.432048Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>id05314</td>\n",
       "      <td>it was a passion that had grown with his growt...</td>\n",
       "      <td>139</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id10083</td>\n",
       "      <td>if this rule were always observed if no man al...</td>\n",
       "      <td>310</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>7.160000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id05326</td>\n",
       "      <td>a poet having very unusual pecuniary resources...</td>\n",
       "      <td>263</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>7.090909</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id02106</td>\n",
       "      <td>and because mere walls and windows must soon d...</td>\n",
       "      <td>259</td>\n",
       "      <td>49</td>\n",
       "      <td>28</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>id03741</td>\n",
       "      <td>the news spread through athens and the whole c...</td>\n",
       "      <td>189</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>5.823529</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 processed  length  words  \\\n",
       "id                                                                          \n",
       "id05314  it was a passion that had grown with his growt...     139     27   \n",
       "id10083  if this rule were always observed if no man al...     310     51   \n",
       "id05326  a poet having very unusual pecuniary resources...     263     45   \n",
       "id02106  and because mere walls and windows must soon d...     259     49   \n",
       "id03741  the news spread through athens and the whole c...     189     34   \n",
       "\n",
       "         words_not_stopword  avg_word_length  commas  \n",
       "id                                                    \n",
       "id05314                  11         6.000000       0  \n",
       "id10083                  25         7.160000       4  \n",
       "id05326                  22         7.090909       8  \n",
       "id02106                  28         5.285714       1  \n",
       "id03741                  17         5.823529       5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features=[c for c in df.columns.values if c not in [\"id\",\"text\",\"author\"]]\n",
    "numeric_features=[c for c in df.columns.values if c not in [\"id\",\"text\",\"author\",\"processed\"]]\n",
    "target=\"author\"\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(df[features],df[target],test_size=1/3,random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The standard preprocessing apply the same preprocessing to the whole dataset, but in cases where we have heterogeneous data, this doesn't quite work. So first thing to do is create a selector transformer that simply returns the one column in the dataset by the key value I pass.\n",
    "\n",
    "I was having difficulty getting the selector to play nicely, so I made two different selectors for either text or numeric columns. The return type is different, but other than that they work the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:05:30.554712Z",
     "start_time": "2020-04-29T08:05:30.548061Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self,key):\n",
    "        self.key=key\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self,key):\n",
    "        self.key=key\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To see how this is used, let's actually run it on one column.\n",
    "\n",
    "I'm going to call it on the text column and transform it with another step. But again, pipelines are all about encapsulating several steps, so I'm going to make a mini pipeline that consists of two steps: first grab just that column from the dataset, then perform tf-idf on just that column and return the results.\n",
    "\n",
    "To make a pipeline, just pass an array of tuples of the format (name, object). The first part is the name of the action, and the second is the actual object. So this pipeline consists of \"selecting\" and then \"tfidf-ing\" a column.\n",
    "\n",
    "To execute, use it just like any other transformer. Call text.fit() to fit to training data, text.transform() to apply it to training data, or text.fit_transform() to do both.\n",
    "\n",
    "Since it's text, it will return a sparse matrix, but we can see that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:09:13.492989Z",
     "start_time": "2020-04-29T08:09:13.186881Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13052x21455 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 147424 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text=Pipeline([\n",
    "    (\"selector\",TextSelector(key=\"processed\")),\n",
    "    (\"tfidf\",TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since our data is heterogeneous, we might want to do something else on numeric data, so let's build a mini pipeline for that too.\n",
    "\n",
    "This transformer will be a simple scaler. Since our data is mixed, we must apply it column by column. Let's make one to process the \"length\" variable I made above. Just like the text one, we combine two steps, first selecting the column, then transforming the column, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:11:03.304673Z",
     "start_time": "2020-04-29T08:11:03.293795Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06153889],\n",
       "       [ 1.52952007],\n",
       "       [ 1.09221147],\n",
       "       ...,\n",
       "       [-0.46162975],\n",
       "       [-0.14527884],\n",
       "       [-0.39649868]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "length=Pipeline([\n",
    "    (\"selector\",NumberSelector(key=\"length\")),\n",
    "    (\"standard\",StandardScaler())\n",
    "])\n",
    "\n",
    "length.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that the transformer pipeline returns a matrix for the column it's called on, so now all that's left to do is join the results from several transformed variables into a single dataset.\n",
    "\n",
    "First, let's transform all the numeric columns with the standard scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:15:05.287305Z",
     "start_time": "2020-04-29T08:15:05.277750Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words=Pipeline([\n",
    "    (\"selector\",NumberSelector(key=\"words\")),\n",
    "    (\"standard\",StandardScaler())\n",
    "])\n",
    "words_not_stopword=Pipeline([\n",
    "    (\"selector\",NumberSelector(key=\"words_not_stopword\")),\n",
    "    (\"standard\",StandardScaler())\n",
    "])\n",
    "avg_word_length=Pipeline([\n",
    "    (\"selector\",NumberSelector(key=\"avg_word_length\")),\n",
    "    (\"standard\",StandardScaler())\n",
    "])\n",
    "commas=Pipeline([\n",
    "    (\"selector\",NumberSelector(key=\"commas\")),\n",
    "    (\"standard\",StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To make a pipeline from all of our pipelines, we do the same thing, but now we use a FeatureUnion to join the feature processing pipelines.\n",
    "\n",
    "The syntax is the same as a regular pipeline, it's just an array of tuple, with the (name, object) format.\n",
    "\n",
    "The feature union itself is not a pipeline, it's just a union, so we need to do one more step to make it useable: pass it to a pipeline, with the same structure, an array of tuples, with the simple (name, object) format.\n",
    "\n",
    "We can then apply all those transformations at once with a single fit, transform, or fit_transform call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:18:43.280093Z",
     "start_time": "2020-04-29T08:18:42.959549Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13052x21460 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 212684 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats=FeatureUnion([\n",
    "    (\"text\",text),\n",
    "    (\"length\",length),\n",
    "    (\"words\",words),\n",
    "    (\"words_not_stopword\",words_not_stopword),\n",
    "    (\"avg_word_length\",avg_word_length),\n",
    "    (\"commas\",commas)])\n",
    "\n",
    "feature_processing=Pipeline([(\"feats\",feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To add a model to the mix and generate predictions as well, we can add a model at the end of the pipeline. The syntax is, you guessed it, an array of tuples, merging the transformations with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:26:45.975976Z",
     "start_time": "2020-04-29T08:26:31.012460Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675808181400337"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline=Pipeline([\n",
    "    (\"features\",feats),\n",
    "    (\"classifier\",RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred=pipeline.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <a name=\"4\">Cross Validation To Find The Best Pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What if I wanted to do cross validation on my pipeline? How many trees should I use on my classifier? How deep should I go? Or even more complicated, how many words should I use in my tf-idf transform? Should I include stop words? Pipelines allow us to do that with just a few more lines.\n",
    "\n",
    "Cross validation is all about figuring out what the best hyperparameters of the data set is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:30:28.923778Z",
     "start_time": "2020-04-29T08:30:28.916875Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'features', 'classifier', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__verbose', 'features__text', 'features__length', 'features__words', 'features__words_not_stopword', 'features__avg_word_length', 'features__commas', 'features__text__memory', 'features__text__steps', 'features__text__verbose', 'features__text__selector', 'features__text__tfidf', 'features__text__selector__key', 'features__text__tfidf__analyzer', 'features__text__tfidf__binary', 'features__text__tfidf__decode_error', 'features__text__tfidf__dtype', 'features__text__tfidf__encoding', 'features__text__tfidf__input', 'features__text__tfidf__lowercase', 'features__text__tfidf__max_df', 'features__text__tfidf__max_features', 'features__text__tfidf__min_df', 'features__text__tfidf__ngram_range', 'features__text__tfidf__norm', 'features__text__tfidf__preprocessor', 'features__text__tfidf__smooth_idf', 'features__text__tfidf__stop_words', 'features__text__tfidf__strip_accents', 'features__text__tfidf__sublinear_tf', 'features__text__tfidf__token_pattern', 'features__text__tfidf__tokenizer', 'features__text__tfidf__use_idf', 'features__text__tfidf__vocabulary', 'features__length__memory', 'features__length__steps', 'features__length__verbose', 'features__length__selector', 'features__length__standard', 'features__length__selector__key', 'features__length__standard__copy', 'features__length__standard__with_mean', 'features__length__standard__with_std', 'features__words__memory', 'features__words__steps', 'features__words__verbose', 'features__words__selector', 'features__words__standard', 'features__words__selector__key', 'features__words__standard__copy', 'features__words__standard__with_mean', 'features__words__standard__with_std', 'features__words_not_stopword__memory', 'features__words_not_stopword__steps', 'features__words_not_stopword__verbose', 'features__words_not_stopword__selector', 'features__words_not_stopword__standard', 'features__words_not_stopword__selector__key', 'features__words_not_stopword__standard__copy', 'features__words_not_stopword__standard__with_mean', 'features__words_not_stopword__standard__with_std', 'features__avg_word_length__memory', 'features__avg_word_length__steps', 'features__avg_word_length__verbose', 'features__avg_word_length__selector', 'features__avg_word_length__standard', 'features__avg_word_length__selector__key', 'features__avg_word_length__standard__copy', 'features__avg_word_length__standard__with_mean', 'features__avg_word_length__standard__with_std', 'features__commas__memory', 'features__commas__steps', 'features__commas__verbose', 'features__commas__selector', 'features__commas__standard', 'features__commas__selector__key', 'features__commas__standard__copy', 'features__commas__standard__with_mean', 'features__commas__standard__with_std', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:43:15.116958Z",
     "start_time": "2020-04-29T08:39:08.054008Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters={\"features__text__tfidf__max_df\":[0.9,0.95],\n",
    "                \"features__text__tfidf__ngram_range\":[(1,1),(1,2)],\n",
    "                \"classifier__max_depth\":[50,70],\n",
    "                \"classifier__min_samples_leaf\":[1,2]}\n",
    "\n",
    "clf=GridSearchCV(pipeline,hyperparameters,cv=5)\n",
    "\n",
    "# Fit and tune model\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:45:25.290833Z",
     "start_time": "2020-04-29T08:45:25.212808Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': 70,\n",
       " 'classifier__min_samples_leaf': 2,\n",
       " 'features__text__tfidf__max_df': 0.9,\n",
       " 'features__text__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's call refit to automatically fit the pipeline on all of the training data with the best_params_setting applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T08:49:33.970632Z",
     "start_time": "2020-04-29T08:49:33.375786Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6633981921250192"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refitting on entire training data using best settings\n",
    "clf.refit\n",
    "\n",
    "preds=clf.predict(X_test)\n",
    "probs=clf.predict_proba(X_test)\n",
    "\n",
    "np.mean(preds==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <a name=\"5\">Final Predictions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To generate submission results, we just need to do the preprocessing on the submission data, then call the pipeline with the predict_proba call, since we want to know all the probabilities, not just the label.\n",
    "\n",
    "The only tricky part for the submission is we need the class names as the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:00:42.105062Z",
     "start_time": "2020-04-29T09:00:41.357393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission=pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "submission=processing(submission)\n",
    "predictions=clf.predict_proba(submission)\n",
    "\n",
    "preds=pd.DataFrame(data=predictions,columns=clf.best_estimator_.named_steps[\"classifier\"].classes_)\n",
    "\n",
    "# generating a submission file\n",
    "result=pd.concat([submission[[\"id\"]],preds],axis=1)\n",
    "result.set_index(\"id\",inplace=True)\n",
    "result.head()\n",
    "\n",
    "# result.to_csv(\"output/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
